---
title: 大数据（十三）Sqoop
date: 2020-04-15 19:23:32
tags: [大数据,sqoop]
copyright: true
password:
toc: true
---

传统的应用程序管理系统，即应用程序与使用RDBMS的关系数据库的交互，是产生大数据的来源之一。由RDBMS生成的这种大数据存储在关系数据库结构中的关系数据库服务器中。

当大数据存储和Hadoop生态系统的MapReduce，Hive，HBase，Cassandra，Pig等分析器出现时，他们需要一种工具来与关系数据库服务器进行交互，以导入和导出驻留在其中的大数据。在这里，Sqoop在Hadoop生态系统中占据一席之地，以便在关系数据库服务器和Hadoop的HDFS之间提供可行的交互。

Sqoop(“SQL到Hadoop和Hadoop到SQL):是一个用于在Hadoop和关系数据库服务器之间传输数据的工具。它用于从关系数据库（如MySQL，Oracle）导入数据到Hadoop HDFS，并从Hadoop文件系统导出到关系数据库。它由Apache软件基金会提供。

本文章主要介绍 Sqoop 和 在centos环境下如何安装 。

<!--more-->

## Quick Guide

###  一、流程

![](/image/大数据13/大数据13_001.png)

- Sqoop导入：导入工具从RDBMS向HDFS导入单独的表。表中的每一行都被视为HDFS中的记录。所有记录都以文本文件的形式存储在文本文件中或作为Avro和Sequence文件中的二进制数据存储。
- Sqoop导出：导出工具将一组文件从HDFS导出回RDBMS。给Sqoop输入的文件包含记录，这些记录在表中被称为行。这些被读取并解析成一组记录并用用户指定的分隔符分隔。


### 二、工作机制

将导入或导出命令翻译成 MapReduce 程序来实现 在翻译出的 MapReduce 中主要是对 InputFormat 和 OutputFormat 进行定制

### 三、Sqoop版本

- 1.sqoop的版本sqoop1和sqoop2是两个不同的版本，它们是完全不兼容的
- 2.版本划分方式: apache1.4.X之后的版本是1,1.99.0之上的版本是2
- 3.Sqoop2相比sqoop1的优势有：
	- 1.它引入的sqoop Server，便于集中化的管理Connector或者其它的第三方插件；
	- 2.多种访问方式：CLI、Web UI、REST API；
	- 3.它引入了基于角色的安全机制，管理员可以在sqoop Server上配置不同的角色。
- 4.Sqoop2和sqoop1的功能性对比：
	- 1.它引入的sqoop Server，便于集中化的管理Connector或者其它的第三方插件；
	- 2.多种访问方式：CLI、Web UI、REST API；
	- 3. 它引入了基于角色的安全机制，管理员可以在sqoop Server上配置不同的角色。
- 5.Sqoop2和sqoop1的功能性对比：

![](/image/大数据13/大数据13_002.png)


- 6.Sqoop1和Sqoop2的架构区别：
	- 1.Sqoop1的架构图：

	![](/image/大数据13/大数据13_003.png)
    ```
    版本号：1.4.X以后的Sqoop1
    在架构上：Sqoop1使用Sqoop客户端直接提交代码方式
    访问方式：CLI命令行控制台方式访问
    安全性：命令或者脚本指定用户数据库名和密码
    原理：Sqoop工具接收到客户端的shell命令或者Java api命令后，通过Sqoop中的任务翻译器(Task Translator)将命令转换为对应的MapReduce任务，而后将关系型数据库和Hadoop中的数据进行相互转移，进而完成数据的拷贝
    ```
	- 2.Sqoop2架构图：
	![](/image/大数据13/大数据13_004.png)
    
    ```
	版本号：1.99.X以上的版本Sqoop2   
	在架构上：Sqoop2引入了 Sqoop server,对对connector实现了集中的管理访问方式：REST API、 JAVA API、 WEB UI以及CLI控制台方式进行访问    
    CLI方式访问，会通过交互过程界面，输入的密码信息会被看到，同时Sqoop2引入基亍角色的安全机制，Sqoop2比Sqoop多了一个Server端。
    ```
    - 3.Sqoop1和Sqoop2优缺点：
    ```
    Sqoop1优点：架构部署简单
    Sqoop1缺点：命令行方式容易出错，格式紧耦合，无法支持所有数据类型，安全机制不够完善，例如密码暴漏，安装需要root权限，connector必须符合JDBC模型
    Sqoop2优点：多种交互方式，命令行，web UI，rest API，conncetor集中化管理，所有的链接安装在Sqoop server上，完善权限管理机制，connector规范化，仅仅负责数据的读写
    Sqoop2缺点：Sqoop2的缺点，架构稍复杂，配置部署更繁琐
    ```

### 四、Sqoop1的安装

因为绝大部分企业所使用的sqoop的版本都是 Sqoop1，这边介绍 Sqoop1 的安装。

- 1.前提概述
	- 1.sqoop就是一个工具,只需要在一个节点上进行安装即可。
	- 2.需要跟那个组件打交道，就需要安装的节点上有对应组件。
- 2.下载
	- 1.从[官网](http://sqoop.apache.org/)下载,选择 sqoop-1.4.7 下载。
- 3.上传并解压
```bash
cd /hadoop/software
tar -zxvf sqoop-1.4.7.bin__hadoop-2.6.0.tar.gz  -C /hadopp/install/
```

- 4.配置环境变量
```bash
cd /home/hadoop/.bashrc
vi .bashrc

# 将下面两行添加到末尾
export SQOOP_HOME=/hadoop/install/sqoop-1.4.7
export PATH=$PATH:$SQOOP_HOME/bin

# 使环境变量生效
. .bashrc
```


- 5.修改配置
```bash
cd /hadoop/install/sqoop-1.4.7/conf/
mv sqoop-env-template.sh sqoop-env.sh
vi sqoop-env.sh
```
修改配置内容
```bash
export HADOOP_COMMON_HOME=/install/hadoop/hadoop

#Set path to where hadoop-*-core.jar is available
export HADOOP_MAPRED_HOME=/install/hadoop/hadoop

#set the path to where bin/hbase is available
export HBASE_HOME=/hadoop/install/hbase-2.0.5

#Set the path to where bin/hive is available
export HIVE_HOME=/hadoop/install/apache-hive-3.1.2

#Set the path for where zookeper config dir is
export ZOOCFGDIR=/hadoop/install/zookeeper-3.4.12/conf
```

- 6.增加驱动
	- 1.从[mysql官网](https://downloads.mysql.com/archives/c-j/)下载对于版本驱动
	- 2.把驱动放入sqoop1.4.7/lib 目录下
    ```
    cp /hadoop/software/mysql-connector-java-5.1.38-bin.jar /hadoop/install/sqoop1.4.7/lib/
    ```
- 7.验证安装是否成功
```bash
sqoop-version
```

### 五、Sqoop的基本命令

- 1.列出MySQL数据有哪些数据库

```bash
sqoop list-databases --connect jdbc:mysql://master:3306/ --username root --password root
```

- 2.列出MySQL中的某个数据库有哪些数据表
```
sqoop list-tables --connect jdbc:mysql://master:3306/ --username root --password root
```

- 3.创建一张跟mysql中的tbl_test表一样的hive表tbl_hive：

```bash
sqoop create-hive-table --connect jdbc:mysql://master:3306/ --username root --password root  --table tbl_test  --hive-table tbl_hive
```

- 4.从RDBMS导入到HDFS中

```bash
sqoop import (generic-args) (import-args)
```
常用参数
```
--connect <jdbc-uri> jdbc 连接地址
--connection-manager <class-name> 连接管理者
--connection-param-file <filename> 可选参数
--column-family：列族
--driver <class-name> 驱动类
--hadoop-mapred-home <dir> $HADOOP_MAPRED_HOME
--hbase-table：hbase中的table
--hbase-row-key：指定rowkey
--help help 信息
--password <password> 密码
-P 从命令行输入密码
--table ：需要sqoop的表
--username <username> 账号
--verbose 打印流程信息
```

例子: 导入mysql库中的 tbl_test 的数据到HDFS上

```bash
sqoop import  --connect jdbc:mysql://master:3306/ --username root --password root  --table tbl_test  -m 1
```

- 5.把MySQL数据库中的表数据导入到Hive中

```bash
sqoop import  --connect jdbc:mysql://master:3306/ --username root --password root  --table tbl_test --hive-import -m 1
```

导入过程:

```
第一步：导入mysql.tbl_test的数据到hdfs的默认路径
第二步：自动仿造mysql.help_keyword去创建一张hive表, 创建在默认的default库中
第三步：把临时目录中的数据导入到hive表中
```


- 6.把MySQL数据库中的表数据导入到hbase

```bash
# 需要先创建Hbase里面的表
sqoop import  --connect jdbc:mysql://master:3306/ --username root --password root  --table tbl_test --hbase-table tbl_hbase --column-family person --hbase-row-key tbl_hbase_key
```
